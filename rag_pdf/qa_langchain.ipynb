{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# may be very similar to RAG\n",
    "# credits to this code : https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-qa/question_answering_documents_langchain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our main GPS results show that the strong partisan differences in social distancing behavior\n",
      "that emerged with the rise of COVID-19 are not merely an artifact of differences in public policies\n",
      "or observed risks. Controlling for state-time ﬁxed effects to account for heterogenous policy re-\n",
      "sponses by state governments only attenuates the partisan gap slightly. Including controls to proxy\n",
      "for local policy, health, weather, and economic variables interacted ﬂexibly with time attenuates\n",
      "the gap more substantially, but it remains statistically and economically signiﬁcant. After including\n",
      "our full set of controls, we estimate that moving from the 10th to the 90th percentile of Republican\n",
      "county vote share is associated with 11.5 and 15.2 percent increases in the number of POI visits\n",
      "during the weeks of April 6 and May 11, when social distancing and partisan gaps are at their\n",
      "respective peaks.\n",
      "Our ﬁndings are robust to the inclusion or exclusion of control variables, excluding states with\n",
      "early COVID-19 outbreaks, or dropping highly populated counties. Replacing the continuous mea-\n",
      "sure of partisanship with discrete indicators for portions of the Republican vote share distribution\n",
      "or restricting the sample to counties from certain portions of the distribution does not change our\n",
      "qualitative conclusions. Furthermore, there is no evidence of a similar partisan gap during the\n",
      "same period in 2019 conditional on the same set of controls. We ﬁnd similar evidence of a partisan\n",
      "gap at the voting precinct level, but focus on county-level analyses due to limitations facing the\n",
      "precinct speciﬁcation (see footnote 14).\n",
      "To complement the data showing county-level differences in behavior, we use a nationally-\n",
      "representative survey to show that individual behavior and beliefs about social distancing are par-\n",
      "tisan. We collect participants’ demographics (including party afﬁliation), beliefs regarding the\n",
      "efﬁcacy of social distancing, self-reported distancing due to COVID-19, and predictions about fu-\n",
      "ture COVID-19 cases. Compared to Republicans, we ﬁnd that Democrats believe the pandemic is\n",
      "more severe and report a greater reduction in contact with others. In our survey, we also randomly\n",
      "vary whether predictions about future COVID-19 cases are incentivized. We do not ﬁnd evidence\n",
      "that incentives reduce the partisan gap, suggesting that these predictions are less likely to be due\n",
      "to partisan cheerleading (as in Bullock et al. 2015 and Prior et al. 2015) and more likely to reﬂect\n",
      "true differences in beliefs. These partisan gaps in survey responses emerge even when comparing\n",
      "respondents within the same county.\n",
      "A number of contemporaneous studies also measure partisan differences in responses to COVID-\n",
      "19.1Gadarian et al. (2020), Makridis and Rothwell (2020), and Wu and Huber (2020) show that\n",
      "1Coverage in the media and some studies examine partisan heterogeneity in response to COVID-19 with no or few\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(\"../data/allcott.pdf\")\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Could you give me information about the sample?\"\n",
    "prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "                    not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "                    Context: \\n {context}?\\n\n",
    "                    Question: \\n {question} \\n\n",
    "                    Answer:\n",
    "                  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A without similarity search (i.e. without vector database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total words in the context:  17527\n"
     ]
    }
   ],
   "source": [
    "len_context = 7 \n",
    "len_input = 3\n",
    "context = \"\\n\".join(str(p.page_content) for p in pages[:len_context])\n",
    "print(\"The total words in the context: \", len(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuffing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pvanb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "stuff_answer = stuff_chain(\n",
    "    {\"input_documents\": pages[len_context:len_context+len_input], \"question\": question}, return_only_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'Answer not available in context. The question is asking for information about the sample used in the analysis, but the provided context does not provide enough information to answer the question. Please provide more context or clarify the question.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                    Answer the question as precise as possible using the provided context. \\n\\n\n",
    "                    Context: \\n {context} \\n\n",
    "                    Question: \\n {question} \\n\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"Given the extracted content and the question, create a final answer.\n",
    "If the answer is not contained in the context, say \"answer not available in context. \\n\\n\n",
    "Summaries: \\n {summaries}?\\n\n",
    "Question: \\n {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_qa_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    return_intermediate_steps=True,\n",
    "    question_prompt=question_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": pages, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A with similarity search (see also code first_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Chroma.from_documents(pages, embeddings).as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_index.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_embeddings_outputs = map_reduce_chain(\n",
    "    {\"input_documents\": docs, \"question\": question}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_reduce_embeddings_outputs[\"output_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
